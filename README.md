ğŸ“Œ DescriÃ§Ã£o do Projeto

Este projeto tem como objetivo desenvolver um pipeline de processamento de dados em tempo real utilizando Apache Spark Streaming e Apache Airflow.

A aplicaÃ§Ã£o serÃ¡ responsÃ¡vel por consumir dados em tempo real a partir de um Data Lake, que armazenarÃ¡ dados estruturados e semi-estruturados, como:

âœ… Bancos de dados relacionais (PostgreSQL)
âœ… Bancos de dados NoSQL (MongoDB)
âœ… Arquivos JSON e CSV no sistema de arquivos local

A ingestÃ£o e entrega de dados serÃ£o realizadas com Apache Kafka e Debezium, garantindo alta performance e baixa latÃªncia na transmissÃ£o dos dados.

ğŸ¯ Objetivos

Criar um Data Lake para armazenar dados estruturados e semi-estruturados
Desenvolver aplicaÃ§Ãµes PySpark para consumir e transformar os dados
Implementar Apache Kafka/Debezium para ingestÃ£o e entrega dos dados
Utilizar Apache Airflow para orquestraÃ§Ã£o e monitoramento dos fluxos de ETL

ğŸ”§ Tecnologias Utilizadas

Apache Spark	Processamento distribuÃ­do de dados
Apache Airflow	OrquestraÃ§Ã£o e monitoramento de ETL
Apache Kafka	Streaming de eventos em tempo real
Debezium	Captura de mudanÃ§as em bancos de dados
PostgreSQL	Banco de dados relacional
MongoDB	Banco de dados NoSQL
PySpark	Biblioteca para trabalhar com Spark em Python

ğŸ—ï¸ Arquitetura do Projeto
A arquitetura sugerida para este projeto segue o fluxo abaixo:

1ï¸âƒ£ Os dados sÃ£o armazenados no Data Lake (PostgreSQL, MongoDB, JSON, CSV).
2ï¸âƒ£ O Apache Kafka/Debezium realiza a ingestÃ£o e entrega dos dados em tempo real.
3ï¸âƒ£ O Apache Spark Streaming (PySpark) processa e transforma os dados consumidos.
4ï¸âƒ£ O Apache Airflow agenda, orquestra e monitora os pipelines de ETL.
